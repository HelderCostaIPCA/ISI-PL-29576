[ISI] Trabalho Pr√°tico I: Integra√ß√£o e An√°lise de Dados de Sa√∫de (Reservas e Dadores)
üéØ Objetivo do Projeto
Este reposit√≥rio cont√©m o trabalho pr√°tico desenvolvido no √¢mbito da Unidade Curricular de Integra√ß√£o de Sistemas de Informa√ß√£o (ISI). O foco √© aplicar e experimentar processos de ETL (Extract, Transform and Load) para integrar dados de sa√∫de dispersos em formato JSON, transform√°-los e prepar√°-los para an√°lise.

O objetivo principal √© consolidar datasets de Reservas de Componentes Sangu√≠neos e Dadores (1¬™ Vez e Regulares) por Regi√£o e Entidade, culminando na produ√ß√£o de um dataset limpo e estruturado.

üõ†Ô∏è Tecnologias Utilizadas
Categoria	Ferramenta	Fun√ß√£o no Projeto	
Crit√©rio de Mais-Valia 

ETL	KNIME Analytics Platform	
Desenvolvimento e execu√ß√£o do Job de ETL.

Sim (Ferramenta de suporte a ETL)
Fonte de Dados	JSON	
Formato de dados de entrada brutos (ex: reservas.json).

Sim (Serializa√ß√£o de JSON)
Destino de Dados	Parquet	Formato de ficheiro otimizado para a carga e an√°lise posterior (DadosDadiva.parquet).	Sim (Diversidade de formatos)
Visualiza√ß√£o	Microsoft Power BI / Similar	
Cria√ß√£o de dashboards para an√°lise dos resultados.

Sim (Visualiza√ß√£o dos resultados)

Exportar para Sheets
‚öôÔ∏è Processo ETL (KNIME Workflow)
O fluxo de trabalho foi constru√≠do no KNIME e est√° dividido em tr√™s fases principais, conforme os princ√≠pios de ETL:

1. Extract & Initial Transformation

Leitura JSON: Leitura das diferentes fontes JSON (Reservas e Dadores).

JSON Path: Extra√ß√£o de campos aninhados como Entidade, Regi√£o, Reservas, Periodo, Latitude e Longitude.

Ungroup: Desagrupamento das colunas de lista para formar linhas at√≥micas.

Filtragem: Aplica√ß√£o de filtros por valores nominais na coluna Regi√£o.

2. Transform & Integrate
Manipula√ß√£o de Data: O campo Periodo (formato YYYY-MM) √© manipulado usando String Manipulation (join($Periodo$, "-01")) e convertido para o tipo Date&Time.

Joins: Integra√ß√£o das pipelines de Reservas e Dadores atrav√©s de um n√≥ Joiner com a correspond√™ncia dupla Entidade AND Regi√£o.

Tratamento de Missing Values: Substitui√ß√£o de valores nulos em colunas num√©ricas (Float e Integer) por zero (0).

3. Load
Parquet Writer: Carregamento dos dados limpos e integrados para o ficheiro DadosDadiva.parquet.

üìä Resultados da An√°lise (Visualiza√ß√µes)
O dataset final processado foi utilizado para gerar visualiza√ß√µes que permitem auditar a distribui√ß√£o de reservas e dadores:

Visualiza√ß√£o	Destaque	Imagem de Refer√™ncia
Soma de Reservas por Regi√£o	Demonstra o volume total de reservas, onde a Entidade Central (IPST, IP) √© a dominante.	[image_5793a5.png]
Soma de Dadores por Regi√£o	Compara a distribui√ß√£o de dadores 1a Vez e Regulares, confirmando a preval√™ncia da Entidade Central.	[image_5793e4.png]

üöÄ Trabalhos Futuros

Orquestra√ß√£o: Explorar ferramentas como Node-RED ou Apache Airflow para agendamento e monitoriza√ß√£o do Job de ETL (Job Control).

Express√µes Regulares (ER): Introduzir o uso de ER para normaliza√ß√£o e limpeza mais robusta dos campos Regi√£o e Entidade.

Desenvolvido por:H√©lder Costa
Integra√ß√£o de API: Explorar o acesso a APIs remotas para enriquecer os dados extra√≠dos.

Desenvolvido por: [Nome do Aluno]
